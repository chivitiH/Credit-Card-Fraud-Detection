"""
Model Evaluation Pipeline - Credit Card Fraud Detection
VERSION CORRIG√âE: √âvaluation finale sur test set JAMAIS VU
M√©triques business r√©alistes et visualisations
"""

import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.metrics import (
    confusion_matrix, roc_auc_score, average_precision_score, 
    roc_curve, precision_recall_curve, accuracy_score, 
    f1_score, precision_score, recall_score
)
import pickle
import json
import os
from pathlib import Path
from typing import Dict, Tuple
import warnings
warnings.filterwarnings('ignore')

def convert_numpy_types(obj):
    """Conversion types NumPy vers Python pour JSON"""
    if isinstance(obj, (np.integer, np.int64)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64)):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    else:
        return obj

class FraudModelEvaluator:
    """üéØ √âvaluateur FINAL - Test set JAMAIS VU pendant training"""
    
    def __init__(self):
        # Chemins
        self.base_dir = Path(__file__).resolve().parent.parent
        self.models_dir = self.base_dir / "models"
        self.processed_dir = self.base_dir / "data" / "processed"
        self.assets_dir = self.base_dir / "assets"
        
        # Cr√©ation dossier assets
        os.makedirs(self.assets_dir, exist_ok=True)
        
        # Donn√©es et mod√®le
        self.model = None
        self.X_test = None
        self.y_test = None
        self.y_pred = None
        self.y_pred_proba = None
        self.feature_names = None
        self.optimal_threshold = 0.5
        
        # Historique training pour comparaison
        self.training_history = None
    
    def load_model_and_test_data(self):
        """
        üéØ CRUCIAL: Chargement mod√®le + TEST SET ISOL√â
        Premier contact du mod√®le avec ces donn√©es
        """
        print("üì• Chargement mod√®le et TEST SET JAMAIS VU...")
        
        # Chargement mod√®le
        model_path = self.models_dir / "fraud_detector.pkl"
        if not model_path.exists():
            raise FileNotFoundError(f"Mod√®le non trouv√©: {model_path}")
        
        with open(model_path, 'rb') as f:
            self.model = pickle.load(f)
        
        # üéØ CRITIQUE: Chargement TEST SET - PREMIER CONTACT
        try:
            self.X_test = pd.read_pickle(self.processed_dir / "X_test.pkl")
            self.y_test = pd.read_pickle(self.processed_dir / "y_test.pkl")
        except FileNotFoundError:
            raise FileNotFoundError(
                "Test set non trouv√©! Ex√©cutez d'abord le preprocessing corrig√©."
            )
        
        # Chargement historique training pour comparaison
        history_path = self.models_dir / "training_history.json"
        if history_path.exists():
            with open(history_path, 'r') as f:
                self.training_history = json.load(f)
        
        self.feature_names = self.X_test.columns.tolist()
        
        print(f"‚úÖ Chargement termin√©:")
        print(f"   ‚Ä¢ Mod√®le: {type(self.model).__name__}")
        print(f"   üîí Test samples (JAMAIS VUS): {len(self.X_test):,}")
        print(f"   ‚Ä¢ Features: {len(self.feature_names)}")
        print(f"   ‚Ä¢ Test fraud rate: {self.y_test.mean()*100:.2f}%")
        print(f"   üéØ PREMI√àRE √âVALUATION sur donn√©es inconnues")
    
    def find_optimal_threshold_and_predict(self):
        """
        üéØ PREMI√àRE PR√âDICTION sur test set + recherche seuil optimal
        """
        print("üéØ PREMI√àRE PR√âDICTION sur test set + seuil optimal...")
        
        # üî• MOMENT DE V√âRIT√â: Premi√®res pr√©dictions sur donn√©es jamais vues
        self.y_pred_proba = self.model.predict_proba(self.X_test)[:, 1]
        
        # Recherche seuil optimal pour F1-Score sur TEST SET
        precisions, recalls, thresholds = precision_recall_curve(self.y_test, self.y_pred_proba)
        f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)
        
        # Seuil optimal (excluant le dernier point)
        optimal_idx = np.argmax(f1_scores[:-1])
        self.optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5
        
        # Pr√©dictions finales avec seuil optimal
        self.y_pred = (self.y_pred_proba >= self.optimal_threshold).astype(int)
        
        print(f"‚úÖ PREMI√àRES PR√âDICTIONS termin√©es:")
        print(f"   ‚Ä¢ Seuil optimal: {self.optimal_threshold:.4f}")
        print(f"   ‚Ä¢ F1-Score optimal: {f1_scores[optimal_idx]:.4f}")
        print(f"   ‚Ä¢ Pr√©dictions g√©n√©r√©es: {len(self.y_pred):,}")
    
    def calculate_final_metrics(self) -> Dict:
        """
        üéØ M√âTRIQUES FINALES - Performance r√©elle du mod√®le
        """
        print("üìä Calcul M√âTRIQUES FINALES sur test set...")
        
        # M√©triques classification
        accuracy = accuracy_score(self.y_test, self.y_pred)
        precision = precision_score(self.y_test, self.y_pred)
        recall = recall_score(self.y_test, self.y_pred)
        f1 = f1_score(self.y_test, self.y_pred)
        
        # M√©triques probabilistes
        roc_auc = roc_auc_score(self.y_test, self.y_pred_proba)
        pr_auc = average_precision_score(self.y_test, self.y_pred_proba)
        
        # Confusion matrix
        cm = confusion_matrix(self.y_test, self.y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        # M√©triques d√©riv√©es
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        npv = tn / (tn + fn) if (tn + fn) > 0 else 0
        
        # üí∞ M√âTRIQUES BUSINESS R√âALISTES
        cost_fp = fp * 50    # ‚Ç¨50 par fausse alerte (investigation)
        cost_fn = fn * 2500  # ‚Ç¨2500 par fraude manqu√©e (perte moyenne)
        total_cost = cost_fp + cost_fn
        cost_per_transaction = total_cost / len(self.y_test)
        
        # √âconomies vs baseline (sans mod√®le)
        baseline_cost = len(self.y_test[self.y_test == 1]) * 2500  # Toutes fraudes passent
        savings = baseline_cost - total_cost
        roi_ratio = savings / (cost_fp + 10000) if (cost_fp + 10000) > 0 else 0  # +10k co√ªt d√©ploiement
        
        # M√©triques avanc√©es
        balanced_accuracy = (recall + specificity) / 2
        matthews_corr = ((tp * tn) - (fp * fn)) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) if ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) > 0 else 0
        
        metrics = {
            'test_set_info': {
                'total_transactions': len(self.y_test),
                'fraud_transactions': int(self.y_test.sum()),
                'normal_transactions': int((self.y_test == 0).sum()),
                'fraud_rate': float(self.y_test.mean())
            },
            'optimal_threshold': self.optimal_threshold,
            'classification_metrics': {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'specificity': specificity,
                'f1_score': f1,
                'balanced_accuracy': balanced_accuracy,
                'matthews_correlation': matthews_corr,
                'npv': npv
            },
            'probabilistic_metrics': {
                'roc_auc': roc_auc,
                'pr_auc': pr_auc
            },
            'confusion_matrix': {
                'true_negative': int(tn),
                'false_positive': int(fp),
                'false_negative': int(fn),
                'true_positive': int(tp)
            },
            'business_impact': {
                'frauds_detected': int(tp),
                'frauds_missed': int(fn),
                'false_alerts': int(fp),
                'cost_false_positive_eur': float(cost_fp),
                'cost_false_negative_eur': float(cost_fn),
                'total_cost_eur': float(total_cost),
                'cost_per_transaction_eur': float(cost_per_transaction),
                'baseline_cost_eur': float(baseline_cost),
                'savings_eur': float(savings),
                'roi_ratio': float(roi_ratio),
                'detection_rate_percent': float(recall * 100),
                'false_positive_rate_percent': float((fp / (fp + tn)) * 100) if (fp + tn) > 0 else 0,
                'precision_percent': float(precision * 100)
            }
        }
        
        # üîç Comparaison avec validation training si disponible
        if self.training_history and 'honest_validation' in self.training_history:
            val_metrics = self.training_history['honest_validation']
            metrics['training_comparison'] = {
                'validation_f1': val_metrics.get('f1_score', 0),
                'test_f1': f1,
                'f1_difference': f1 - val_metrics.get('f1_score', 0),
                'validation_roc_auc': val_metrics.get('roc_auc', 0),
                'test_roc_auc': roc_auc,
                'roc_auc_difference': roc_auc - val_metrics.get('roc_auc', 0),
                'generalization_quality': 'good' if abs(f1 - val_metrics.get('f1_score', 0)) < 0.05 else 'concerning'
            }
        
        print("‚úÖ M√©triques finales calcul√©es")
        return metrics
    
    def create_performance_visualizations(self) -> Dict[str, go.Figure]:
        """Cr√©ation visualisations performance finale"""
        print("üé® Cr√©ation visualisations performance finale...")
        
        visualizations = {}
        
        # 1. Confusion Matrix
        cm = confusion_matrix(self.y_test, self.y_pred)
        cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
        
        annotations = []
        for i in range(2):
            for j in range(2):
                annotations.append({
                    'x': j, 'y': i,
                    'text': f'{cm[i,j]:,}<br>({cm_percent[i,j]:.1f}%)',
                    'showarrow': False,
                    'font': {'color': 'white' if cm[i,j] > cm.max()/2 else 'black', 'size': 16}
                })
        
        cm_fig = go.Figure(data=go.Heatmap(
            z=cm,
            x=['Pr√©dit Normal', 'Pr√©dit Fraude'],
            y=['R√©el Normal', 'R√©el Fraude'],
            colorscale='Blues',
            showscale=True,
            colorbar=dict(title="Nombre")
        ))
        
        cm_fig.update_layout(
            title="üéØ Confusion Matrix - Performance Test Set Final",
            annotations=annotations,
            width=600, height=500,
            xaxis_title="Pr√©diction",
            yaxis_title="R√©alit√©"
        )
        visualizations['confusion_matrix'] = cm_fig
        
        # 2. ROC et Precision-Recall Curves
        fpr, tpr, _ = roc_curve(self.y_test, self.y_pred_proba)
        roc_auc = roc_auc_score(self.y_test, self.y_pred_proba)
        
        precision, recall, _ = precision_recall_curve(self.y_test, self.y_pred_proba)
        pr_auc = average_precision_score(self.y_test, self.y_pred_proba)
        
        curves_fig = make_subplots(
            rows=1, cols=2,
            subplot_titles=[f'ROC Curve (AUC={roc_auc:.3f})', f'Precision-Recall (AUC={pr_auc:.3f})']
        )
        
        # ROC
        curves_fig.add_trace(
            go.Scatter(x=fpr, y=tpr, name=f'ROC (AUC={roc_auc:.3f})', 
                      line=dict(color='blue', width=3)),
            row=1, col=1
        )
        curves_fig.add_trace(
            go.Scatter(x=[0, 1], y=[0, 1], name='Random', 
                      line=dict(dash='dash', color='gray')),
            row=1, col=1
        )
        
        # PR
        curves_fig.add_trace(
            go.Scatter(x=recall, y=precision, name=f'PR (AUC={pr_auc:.3f})', 
                      line=dict(color='red', width=3)),
            row=1, col=2
        )
        
        baseline_pr = sum(self.y_test) / len(self.y_test)
        curves_fig.add_trace(
            go.Scatter(x=[0, 1], y=[baseline_pr, baseline_pr], 
                      name=f'Baseline ({baseline_pr:.3f})', 
                      line=dict(dash='dash', color='gray')),
            row=1, col=2
        )
        
        curves_fig.update_xaxes(title_text="False Positive Rate", row=1, col=1)
        curves_fig.update_yaxes(title_text="True Positive Rate", row=1, col=1)
        curves_fig.update_xaxes(title_text="Recall", row=1, col=2)
        curves_fig.update_yaxes(title_text="Precision", row=1, col=2)
        
        curves_fig.update_layout(height=400, showlegend=True, 
                                title_text="üéØ Performance Curves - Test Set Final")
        visualizations['performance_curves'] = curves_fig
        
        # 3. Feature Importance
        feature_importance = self.model.feature_importances_
        feature_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': feature_importance
        }).sort_values('importance', ascending=False)
        
        top_features = feature_df.head(15)
        
        feat_fig = px.bar(
            top_features,
            x='importance',
            y='feature',
            orientation='h',
            title='üéØ Top 15 Features - Importance Mod√®le Final',
            labels={'importance': 'Importance', 'feature': 'Features'},
            color='importance',
            color_continuous_scale='viridis'
        )
        
        feat_fig.update_layout(
            height=600, 
            yaxis={'categoryorder': 'total ascending'},
            showlegend=False
        )
        visualizations['feature_importance'] = feat_fig
        
        # 4. Distribution Probabilit√©s
        prob_normal = self.y_pred_proba[self.y_test == 0]
        prob_fraud = self.y_pred_proba[self.y_test == 1]
        
        prob_fig = go.Figure()
        
        prob_fig.add_trace(go.Histogram(
            x=prob_normal, 
            name='Transactions Normales',
            opacity=0.7,
            nbinsx=50,
            marker_color='lightblue',
            histnorm='probability'
        ))
        
        prob_fig.add_trace(go.Histogram(
            x=prob_fraud,
            name='Transactions Frauduleuses', 
            opacity=0.7,
            nbinsx=50,
            marker_color='red',
            histnorm='probability'
        ))
        
        prob_fig.add_vline(
            x=self.optimal_threshold, 
            line_dash="dash", 
            line_color="green",
            annotation_text=f"Seuil Optimal: {self.optimal_threshold:.3f}"
        )
        
        prob_fig.update_layout(
            title='üéØ Distribution Probabilit√©s - Test Set Final',
            xaxis_title='Probabilit√© Fraude',
            yaxis_title='Densit√©',
            barmode='overlay',
            height=400
        )
        visualizations['probability_distribution'] = prob_fig
        
        print("‚úÖ Visualisations cr√©√©es")
        return visualizations
    
    def generate_final_business_report(self, metrics: Dict) -> str:
        """üéØ G√©n√©ration rapport business FINAL"""
        cm = metrics['confusion_matrix']
        biz = metrics['business_impact']
        cls = metrics['classification_metrics']
        prob = metrics['probabilistic_metrics']
        
        # Comparaison avec validation si disponible
        comparison_section = ""
        if 'training_comparison' in metrics:
            tc = metrics['training_comparison']
            comparison_section = f"""
## üîç VALIDATION TRAINING vs TEST FINAL

### Comparaison Performance
- **F1-Score Validation**: {tc['validation_f1']:.4f}
- **F1-Score Test Final**: {tc['test_f1']:.4f}
- **Diff√©rence**: {tc['f1_difference']:+.4f}
- **Qualit√© G√©n√©ralisation**: {tc['generalization_quality'].upper()}

### Analyse G√©n√©ralisation
{('‚úÖ EXCELLENTE - Le mod√®le g√©n√©ralise bien sur donn√©es inconnues' if tc['generalization_quality'] == 'good' else '‚ö†Ô∏è ATTENTION - Possible overfitting ou shift de donn√©es')}
"""

        report = f"""
# üõ°Ô∏è RAPPORT FINAL - FRAUD DETECTION MODEL

## üìä R√âSUM√â EX√âCUTIF - PERFORMANCE TEST SET

**üéØ PREMI√àRE √âVALUATION SUR DONN√âES JAMAIS VUES**

### Performance Globale
- **Accuracy**: {cls['accuracy']*100:.1f}%
- **Taux D√©tection Fraudes**: {cls['recall']*100:.1f}%
- **Pr√©cision**: {cls['precision']*100:.1f}%
- **Score F1**: {cls['f1_score']*100:.1f}%
- **ROC-AUC**: {prob['roc_auc']:.3f}
- **Seuil Optimal**: {metrics['optimal_threshold']:.4f}

### üéØ √âchantillon Test
- **Total Transactions**: {metrics['test_set_info']['total_transactions']:,}
- **Fraudes R√©elles**: {metrics['test_set_info']['fraud_transactions']:,}
- **Taux Fraude**: {metrics['test_set_info']['fraud_rate']*100:.2f}%

## üí∞ IMPACT BUSINESS R√âEL

### Performance D√©tection
- **Fraudes D√©tect√©es**: {biz['frauds_detected']:,} / {metrics['test_set_info']['fraud_transactions']:,}
- **Fraudes Manqu√©es**: {biz['frauds_missed']:,}
- **Fausses Alertes**: {biz['false_alerts']:,}
- **Taux D√©tection**: {biz['detection_rate_percent']:.1f}%

### Impact Financier
- **Co√ªt Fausses Alertes**: ‚Ç¨{biz['cost_false_positive_eur']:,.0f}
- **Co√ªt Fraudes Manqu√©es**: ‚Ç¨{biz['cost_false_negative_eur']:,.0f}
- **Co√ªt Total**: ‚Ç¨{biz['total_cost_eur']:,.0f}
- **Co√ªt par Transaction**: ‚Ç¨{biz['cost_per_transaction_eur']:.3f}

### ROI et √âconomies
- **Co√ªt Sans Mod√®le**: ‚Ç¨{biz['baseline_cost_eur']:,.0f}
- **√âconomies R√©alis√©es**: ‚Ç¨{biz['savings_eur']:,.0f}
- **ROI**: {biz['roi_ratio']:.1f}:1
- **R√©duction Pertes**: {(biz['savings_eur']/biz['baseline_cost_eur']*100):.1f}%

{comparison_section}

## üéØ PERFORMANCE TECHNIQUE D√âTAILL√âE

### M√©triques Classification
- **Accuracy**: {cls['accuracy']:.4f}
- **Precision**: {cls['precision']:.4f}
- **Recall (Sensitivity)**: {cls['recall']:.4f}
- **Specificity**: {cls['specificity']:.4f}
- **F1-Score**: {cls['f1_score']:.4f}
- **Balanced Accuracy**: {cls['balanced_accuracy']:.4f}
- **Matthews Correlation**: {cls['matthews_correlation']:.4f}
- **NPV**: {cls['npv']:.4f}

### M√©triques Probabilistes
- **ROC-AUC**: {prob['roc_auc']:.4f}
- **PR-AUC**: {prob['pr_auc']:.4f}

### Matrice de Confusion
```
                  Pr√©dictions
R√©alit√©    Normal    Fraude
Normal     {cm['true_negative']:,}      {cm['false_positive']:,}
Fraude     {cm['false_negative']:,}       {cm['true_positive']:,}
```

## ‚úÖ VALIDATION PRODUCTION REQUIREMENTS

| Requirement | Target | Test Final | Status |
|-------------|--------|------------|--------|
| F1-Score    | >75%   | {cls['f1_score']*100:.1f}% | {'‚úÖ' if cls['f1_score'] > 0.75 else 'üî¥'} |
| Precision   | >80%   | {cls['precision']*100:.1f}% | {'‚úÖ' if cls['precision'] > 0.80 else 'üî¥'} |
| Recall      | >70%   | {cls['recall']*100:.1f}% | {'‚úÖ' if cls['recall'] > 0.70 else 'üî¥'} |
| ROC-AUC     | >90%   | {prob['roc_auc']*100:.1f}% | {'‚úÖ' if prob['roc_auc'] > 0.90 else 'üî¥'} |
| FP Rate     | <5%    | {biz['false_positive_rate_percent']:.1f}% | {'‚úÖ' if biz['false_positive_rate_percent'] < 5.0 else 'üî¥'} |

## üöÄ VERDICT FINAL

### Statut Production
{self._get_production_verdict(cls, prob, biz)}

### Recommandations Business
{self._get_business_recommendations(cls, prob, biz)}

### Impact Attendu en Production
- **R√©duction Fraudes**: {biz['detection_rate_percent']:.0f}% des cas d√©tect√©s
- **√âconomies Annuelles**: ‚Ç¨{biz['savings_eur']*12:,.0f} (extrapolation mensuelle)
- **Am√©lioration Op√©rationnelle**: {(100-biz['false_positive_rate_percent']):.1f}% transactions l√©gitimes non impact√©es
- **Conformit√©**: Aide au respect PCI DSS et r√©glementations

### Limites et Consid√©rations
- **Donn√©es Test**: Performance sur √©chantillon sp√©cifique
- **Adaptation**: Monitoring performance requis en production
- **Seuil**: Ajustable selon contexte business
- **√âvolution**: R√©entra√Ænement p√©riodique recommand√©

---

**üéØ Rapport g√©n√©r√© sur TEST SET FINAL - Premi√®re √©valuation**
*Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*
*√âchantillon: {metrics['test_set_info']['total_transactions']:,} transactions jamais vues*
        """
        
        return report
    
    def _get_production_verdict(self, cls, prob, biz) -> str:
        """Verdict production bas√© sur m√©triques finales"""
        production_ready = True
        issues = []
        
        if cls['f1_score'] < 0.75:
            production_ready = False
            issues.append("F1-Score insuffisant")
        
        if cls['precision'] < 0.80:
            production_ready = False
            issues.append("Pr√©cision insuffisante")
            
        if cls['recall'] < 0.70:
            production_ready = False
            issues.append("Recall insuffisant")
            
        if prob['roc_auc'] < 0.90:
            production_ready = False
            issues.append("ROC-AUC insuffisant")
        
        if biz['false_positive_rate_percent'] > 5.0:
            production_ready = False
            issues.append("Taux faux positifs trop √©lev√©")
        
        if production_ready:
            return """
üü¢ **PRODUCTION READY - D√âPLOIEMENT RECOMMAND√â**
- Toutes les m√©triques respectent les seuils production
- Performance stable sur donn√©es inconnues
- ROI positif confirm√©
- Impact business significatif"""
        else:
            return f"""
üü° **OPTIMISATION REQUISE AVANT PRODUCTION**
- Issues identifi√©es: {', '.join(issues)}
- Performance acceptable mais am√©liorable
- Consid√©rer r√©entra√Ænement ou tuning
- √âvaluation co√ªt/b√©n√©fice n√©cessaire"""
    
    def _get_business_recommendations(self, cls, prob, biz) -> str:
        """Recommandations business bas√©es sur performance"""
        recommendations = []
        
        if cls['recall'] < 0.80:
            recommendations.append("‚Ä¢ **Am√©liorer D√©tection**: Ajuster seuil ou enrichir features")
        
        if cls['precision'] < 0.85:
            recommendations.append("‚Ä¢ **R√©duire Fausses Alertes**: Optimiser seuil de d√©cision")
        
        if biz['roi_ratio'] < 5:
            recommendations.append("‚Ä¢ **Optimiser ROI**: R√©viser co√ªts d'investigation")
        
        if prob['roc_auc'] < 0.95:
            recommendations.append("‚Ä¢ **Performance Technique**: Explorer ensemble methods")
        
        recommendations.extend([
            "‚Ä¢ **Monitoring**: Surveillance performance hebdomadaire",
            "‚Ä¢ **Adaptation**: R√©entra√Ænement trimestriel recommand√©",
            "‚Ä¢ **Seuil Dynamique**: Ajustement selon contexte m√©tier",
            "‚Ä¢ **Integration**: API temps r√©el pour scoring live"
        ])
        
        return '\n'.join(recommendations)
    
    def save_final_evaluation(self, metrics: Dict, report: str, visualizations: Dict):
        """Sauvegarde √©valuation finale compl√®te"""
        print("üíæ Sauvegarde √©valuation finale...")
        
        # Conversion types NumPy pour JSON
        metrics_json = convert_numpy_types(metrics)
        
        # Sauvegarde m√©triques finales
        with open(self.models_dir / "final_test_metrics.json", 'w', encoding='utf-8') as f:
            json.dump(metrics_json, f, indent=2)
        
        # Sauvegarde rapport business final
        with open(self.assets_dir / "final_evaluation_report.md", 'w', encoding='utf-8') as f:
            f.write(report)
        
        # Sauvegarde visualisations
        try:
            for name, fig in visualizations.items():
                fig.write_image(self.assets_dir / f"final_{name}.png", width=800, height=600)
            print("‚úÖ Visualisations sauvegard√©es")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur sauvegarde visualisations: {e}")
            print("   (Kaleido requis: pip install kaleido)")
        
        # R√©sum√© JSON compact pour int√©gration
        summary = {
            'model_performance': {
                'f1_score': metrics_json['classification_metrics']['f1_score'],
                'precision': metrics_json['classification_metrics']['precision'],
                'recall': metrics_json['classification_metrics']['recall'],
                'roc_auc': metrics_json['probabilistic_metrics']['roc_auc']
            },
            'business_impact': {
                'frauds_detected': metrics_json['business_impact']['frauds_detected'],
                'savings_eur': metrics_json['business_impact']['savings_eur'],
                'roi_ratio': metrics_json['business_impact']['roi_ratio']
            },
            'production_ready': metrics_json['classification_metrics']['f1_score'] > 0.75,
            'evaluation_date': pd.Timestamp.now().isoformat(),
            'test_samples': metrics_json['test_set_info']['total_transactions']
        }
        
        with open(self.assets_dir / "model_summary.json", 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2)
        
        print(f"‚úÖ √âvaluation finale sauvegard√©e:")
        print(f"   ‚Ä¢ M√©triques: models/final_test_metrics.json")
        print(f"   ‚Ä¢ Rapport: assets/final_evaluation_report.md")
        print(f"   ‚Ä¢ R√©sum√©: assets/model_summary.json")
        print(f"   ‚Ä¢ Visualisations: assets/final_*.png")
    
    def display_final_summary(self, metrics: Dict):
        """Affichage r√©sum√© final"""
        print("\n" + "=" * 80)
        print("üéâ √âVALUATION FINALE TERMIN√âE - TEST SET JAMAIS VU")
        print("=" * 80)
        
        cls = metrics['classification_metrics']
        biz = metrics['business_impact']
        prob = metrics['probabilistic_metrics']
        
        # Performance principale
        print("üéØ PERFORMANCE TEST SET FINAL:")
        print(f"   ‚Ä¢ F1-Score: {cls['f1_score']:.4f}")
        print(f"   ‚Ä¢ Precision: {cls['precision']:.4f}")
        print(f"   ‚Ä¢ Recall: {cls['recall']:.4f}")
        print(f"   ‚Ä¢ ROC-AUC: {prob['roc_auc']:.4f}")
        
        # Impact business
        print(f"\nüí∞ IMPACT BUSINESS R√âEL:")
        print(f"   ‚Ä¢ Fraudes d√©tect√©es: {biz['frauds_detected']:,}/{metrics['test_set_info']['fraud_transactions']:,}")
        print(f"   ‚Ä¢ Fausses alertes: {biz['false_alerts']:,}")
        print(f"   ‚Ä¢ √âconomies: ‚Ç¨{biz['savings_eur']:,.0f}")
        print(f"   ‚Ä¢ ROI: {biz['roi_ratio']:.1f}:1")
        
        # Comparaison validation si disponible
        if 'training_comparison' in metrics:
            tc = metrics['training_comparison']
            print(f"\nüîç G√âN√âRALISATION:")
            print(f"   ‚Ä¢ F1 Validation: {tc['validation_f1']:.4f}")
            print(f"   ‚Ä¢ F1 Test Final: {tc['test_f1']:.4f}")
            print(f"   ‚Ä¢ Diff√©rence: {tc['f1_difference']:+.4f}")
            print(f"   ‚Ä¢ Qualit√©: {tc['generalization_quality'].upper()}")
        
        # Verdict final
        production_ready = (cls['f1_score'] > 0.75 and 
                          cls['precision'] > 0.80 and 
                          cls['recall'] > 0.70 and 
                          prob['roc_auc'] > 0.90)
        
        if production_ready:
            print(f"\nüü¢ VERDICT: PRODUCTION READY!")
            print(f"   ‚úÖ Toutes m√©triques conformes")
            print(f"   ‚úÖ ROI positif confirm√©") 
            print(f"   ‚úÖ G√©n√©ralisation valid√©e")
        else:
            print(f"\nüü° VERDICT: OPTIMISATION RECOMMAND√âE")
            print(f"   ‚Ä¢ Performance acceptable mais am√©liorable")
            print(f"   ‚Ä¢ √âvaluation co√ªt/b√©n√©fice n√©cessaire")
        
        print(f"\nüìä √âCHANTILLON TEST:")
        print(f"   ‚Ä¢ {metrics['test_set_info']['total_transactions']:,} transactions JAMAIS VUES")
        print(f"   ‚Ä¢ {metrics['test_set_info']['fraud_transactions']:,} fraudes r√©elles")
        print(f"   ‚Ä¢ Premi√®re √©valuation sur donn√©es inconnues")
        
        print("\nüìÇ ARTEFACTS G√âN√âR√âS:")
        print("   ‚Ä¢ M√©triques d√©taill√©es: models/final_test_metrics.json")
        print("   ‚Ä¢ Rapport business: assets/final_evaluation_report.md")
        print("   ‚Ä¢ Visualisations: assets/final_*.png")
        print("   ‚Ä¢ R√©sum√©: assets/model_summary.json")
        
        print("=" * 80)
    
    def run_final_evaluation(self):
        """üéØ Pipeline √©valuation finale compl√®te"""
        print("üöÄ D√âMARRAGE √âVALUATION FINALE - TEST SET JAMAIS VU")
        print("=" * 80)
        
        try:
            # 1. Chargement mod√®le et test set isol√©
            self.load_model_and_test_data()
            
            # 2. Premi√®res pr√©dictions sur donn√©es inconnues
            self.find_optimal_threshold_and_predict()
            
            # 3. Calcul m√©triques finales
            metrics = self.calculate_final_metrics()
            
            # 4. Cr√©ation visualisations
            visualizations = self.create_performance_visualizations()
            
            # 5. G√©n√©ration rapport business final
            report = self.generate_final_business_report(metrics)
            
            # 6. Sauvegarde compl√®te
            self.save_final_evaluation(metrics, report, visualizations)
            
            # 7. Affichage r√©sum√© final
            self.display_final_summary(metrics)
            
            return True
            
        except Exception as e:
            print(f"\n‚ùå ERREUR √âVALUATION FINALE: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """Ex√©cution √©valuation finale"""
    print("üéØ Credit Card Fraud Detection - √âvaluation Finale")
    print("üîí Test Set Jamais Vu - Premi√®re √âvaluation")
    
    evaluator = FraudModelEvaluator()
    success = evaluator.run_final_evaluation()
    
    if success:
        print("\n‚úÖ √âVALUATION FINALE R√âUSSIE!")
        print("üéØ Mod√®le √©valu√© sur donn√©es jamais vues")
        print("üìä Performance r√©elle confirm√©e")
        print("üöÄ Pr√™t pour d√©cision d√©ploiement")
    else:
        print("\n‚ùå √âVALUATION FINALE √âCHOU√âE!")
        exit(1)

if __name__ == "__main__":
    main()