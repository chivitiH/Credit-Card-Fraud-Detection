"""
Data Preprocessing Pipeline - Credit Card Fraud Detection
Pipeline CORRIGÃ‰ pour Ã©viter le data leakage
RÃˆGLE D'OR: Test set complÃ¨tement isolÃ© dÃ¨s le dÃ©but
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearnex import patch_sklearn  # Intel optimization
import pickle
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Configuration Intel optimization
patch_sklearn()

class FraudDataPreprocessor:
    """PrÃ©processeur donnÃ©es fraud detection - VERSION CORRIGÃ‰E"""
    
    def __init__(self, test_size=0.2, validation_size=0.2, random_state=42):
        """
        NOUVEAU: Triple split pour Ã©viter data leakage
        - Train: 60% (training + hyperparameter optimization)
        - Validation: 20% (validation finale pour tuning)
        - Test: 20% (Ã©valuation finale, JAMAIS touchÃ© pendant training)
        """
        self.test_size = test_size
        self.validation_size = validation_size
        self.random_state = random_state
        self.scaler = StandardScaler()
        
        # Configuration chemins
        self.base_dir = Path(__file__).resolve().parent.parent
        self.data_dir = self.base_dir / "data"
        self.raw_data_path = self.data_dir / "raw" / "creditcard.csv"
        self.processed_dir = self.data_dir / "processed"
        self.models_dir = self.base_dir / "models"
        
        # CrÃ©ation dossiers
        os.makedirs(self.processed_dir, exist_ok=True)
        os.makedirs(self.models_dir, exist_ok=True)
    
    def load_raw_data(self) -> pd.DataFrame:
        """Chargement donnÃ©es brutes avec vÃ©rifications"""
        print("ğŸ“Š Chargement dataset Credit Card Fraud...")
        
        if not self.raw_data_path.exists():
            raise FileNotFoundError(
                f"Dataset non trouvÃ©: {self.raw_data_path}\n"
                "TÃ©lÃ©chargez creditcard.csv depuis Kaggle: "
                "https://www.kaggle.com/mlg-ulb/creditcardfraud"
            )
        
        df = pd.read_csv(self.raw_data_path)
        
        # Validation structure dataset
        expected_columns = ['Time', 'Amount', 'Class'] + [f'V{i}' for i in range(1, 29)]
        missing_cols = set(expected_columns) - set(df.columns)
        
        if missing_cols:
            raise ValueError(f"Colonnes manquantes: {missing_cols}")
        
        print(f"âœ… Dataset chargÃ©: {df.shape}")
        print(f"   â€¢ Transactions totales: {len(df):,}")
        print(f"   â€¢ Fraudes: {df['Class'].sum():,} ({df['Class'].mean()*100:.2f}%)")
        print(f"   â€¢ Features: {len(df.columns)}")
        
        return df
    
    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Nettoyage donnÃ©es"""
        print("ğŸ§¹ Nettoyage des donnÃ©es...")
        
        initial_shape = df.shape
        
        # Suppression doublons
        df_clean = df.drop_duplicates()
        duplicates_removed = initial_shape[0] - df_clean.shape[0]
        
        # VÃ©rification valeurs manquantes
        missing_values = df_clean.isnull().sum().sum()
        
        # VÃ©rification valeurs aberrantes (Amount nÃ©gatif)
        negative_amounts = (df_clean['Amount'] < 0).sum()
        if negative_amounts > 0:
            print(f"âš ï¸ {negative_amounts} montants nÃ©gatifs trouvÃ©s, suppression...")
            df_clean = df_clean[df_clean['Amount'] >= 0]
        
        print(f"âœ… Nettoyage terminÃ©:")
        print(f"   â€¢ Doublons supprimÃ©s: {duplicates_removed}")
        print(f"   â€¢ Valeurs manquantes: {missing_values}")
        print(f"   â€¢ Shape finale: {df_clean.shape}")
        
        return df_clean
    
    def feature_engineering(self, df: pd.DataFrame) -> pd.DataFrame:
        """Engineering features avancÃ©"""
        print("âš™ï¸ Feature engineering...")
        
        df_eng = df.copy()
        
        # Features temporelles
        df_eng['Hour'] = (df_eng['Time'] / 3600) % 24
        df_eng['Day'] = (df_eng['Time'] / 86400) % 7
        df_eng['Is_Weekend'] = df_eng['Day'].isin([5, 6]).astype(int)
        df_eng['Is_Night'] = ((df_eng['Hour'] >= 22) | (df_eng['Hour'] <= 6)).astype(int)
        
        # Features montant
        df_eng['Amount_Log'] = np.log1p(df_eng['Amount'])
        df_eng['Amount_Sqrt'] = np.sqrt(df_eng['Amount'])
        
        # Seuils business (basÃ©s sur analyse exploratoire)
        df_eng['Is_High_Amount'] = (df_eng['Amount'] > 640.90).astype(int)  # 95e percentile
        df_eng['Is_Very_High_Amount'] = (df_eng['Amount'] > 2125.87).astype(int)  # 99e percentile
        df_eng['Is_Low_Amount'] = (df_eng['Amount'] == 0).astype(int)
        
        # Features interactions PCA (basÃ©es sur importance)
        # V14, V4, V12, V10, V17 sont les plus importantes
        df_eng['V14_V4_ratio'] = df_eng['V14'] / (df_eng['V4'] + 1e-8)
        df_eng['V12_V10_ratio'] = df_eng['V12'] / (df_eng['V10'] + 1e-8)
        df_eng['V17_V14_ratio'] = df_eng['V17'] / (df_eng['V14'] + 1e-8)
        
        # AgrÃ©gations features importantes
        important_features = ['V14', 'V4', 'V12', 'V10', 'V17', 'V16', 'V3', 'V11']
        df_eng['Important_Features_Sum'] = df_eng[important_features].sum(axis=1)
        df_eng['Important_Features_Mean'] = df_eng[important_features].mean(axis=1)
        df_eng['Important_Features_Std'] = df_eng[important_features].std(axis=1)
        
        # Features statistiques
        pca_features = [f'V{i}' for i in range(1, 29)]
        df_eng['PCA_Sum'] = df_eng[pca_features].sum(axis=1)
        df_eng['PCA_Mean'] = df_eng[pca_features].mean(axis=1)
        df_eng['PCA_Max'] = df_eng[pca_features].max(axis=1)
        df_eng['PCA_Min'] = df_eng[pca_features].min(axis=1)
        
        new_features = len(df_eng.columns) - len(df.columns)
        print(f"âœ… Features crÃ©Ã©es: {new_features}")
        print(f"   â€¢ Features temporelles: Hour, Day, Is_Weekend, Is_Night")
        print(f"   â€¢ Features montant: Amount_Log, Amount_Sqrt, seuils")
        print(f"   â€¢ Features interactions: ratios PCA importantes")
        print(f"   â€¢ Features agrÃ©gations: sum, mean, std")
        
        return df_eng
    
    def triple_split_data(self, df: pd.DataFrame) -> tuple:
        """
        ğŸ¯ NOUVEAU: Triple split stratifiÃ© pour Ã©viter data leakage
        
        1. Split initial: 80% (train+val) / 20% (test) 
        2. Split train+val: 75% train / 25% validation
        
        RÃ©sultat final:
        - Train: 60% des donnÃ©es totales
        - Validation: 20% des donnÃ©es totales  
        - Test: 20% des donnÃ©es totales (JAMAIS touchÃ©)
        """
        print("âœ‚ï¸ Triple split stratifiÃ© ANTI-LEAKAGE...")
        
        # SÃ©paration features et target
        y = df['Class'].copy()
        X = df.drop(columns=['Class'])
        
        # 1er split: SÃ©parer le test set (20%) du reste (80%)
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y,
            test_size=self.test_size,
            random_state=self.random_state,
            stratify=y
        )
        
        # 2Ã¨me split: Du reste (80%), faire train (75%) et validation (25%)
        # 75% de 80% = 60% du total pour train
        # 25% de 80% = 20% du total pour validation
        validation_size_adjusted = self.validation_size / (1 - self.test_size)
        
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp,
            test_size=validation_size_adjusted,
            random_state=self.random_state,
            stratify=y_temp
        )
        
        print(f"âœ… Triple split terminÃ©:")
        print(f"   â€¢ Train: {X_train.shape}, Fraude: {y_train.mean()*100:.2f}%")
        print(f"   â€¢ Validation: {X_val.shape}, Fraude: {y_val.mean()*100:.2f}%")
        print(f"   â€¢ Test: {X_test.shape}, Fraude: {y_test.mean()*100:.2f}%")
        print(f"   ğŸ”’ Test set ISOLÃ‰ - jamais utilisÃ© pendant training")
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def fit_scaler_on_train_only(self, X_train: pd.DataFrame, X_val: pd.DataFrame, X_test: pd.DataFrame) -> tuple:
        """
        ğŸ¯ CRITIQUE: Normalisation sur train uniquement pour Ã©viter data leakage
        """
        print("ğŸ“ Normalisation ANTI-LEAKAGE (fit sur train uniquement)...")
        
        # Features Ã  normaliser
        features_to_scale = ['Amount', 'Time', 'Amount_Log', 'Amount_Sqrt',
                           'Important_Features_Sum', 'Important_Features_Mean',
                           'PCA_Sum', 'PCA_Mean', 'PCA_Max', 'PCA_Min']
        
        # VÃ©rification prÃ©sence features
        available_features = [f for f in features_to_scale if f in X_train.columns]
        
        if available_features:
            # ğŸ¯ CRUCIAL: Fit UNIQUEMENT sur train
            self.scaler.fit(X_train[available_features])
            
            # Transform sur tous les sets
            X_train_scaled = X_train.copy()
            X_val_scaled = X_val.copy()
            X_test_scaled = X_test.copy()
            
            X_train_scaled[available_features] = self.scaler.transform(X_train[available_features])
            X_val_scaled[available_features] = self.scaler.transform(X_val[available_features])
            X_test_scaled[available_features] = self.scaler.transform(X_test[available_features])
            
            print(f"âœ… Features normalisÃ©es: {len(available_features)}")
            print(f"   ğŸ¯ Scaler fit UNIQUEMENT sur train (anti-leakage)")
            
            return X_train_scaled, X_val_scaled, X_test_scaled
        else:
            print("âš ï¸ Aucune feature Ã  normaliser trouvÃ©e")
            return X_train, X_val, X_test
    
    def save_processed_data(self, X_train, X_val, X_test, y_train, y_val, y_test):
        """Sauvegarde donnÃ©es preprocessÃ©es - VERSION TRIPLE SPLIT"""
        print("ğŸ’¾ Sauvegarde donnÃ©es preprocessÃ©es...")
        
        # Sauvegarde datasets
        X_train.to_pickle(self.processed_dir / "X_train.pkl")
        X_val.to_pickle(self.processed_dir / "X_val.pkl")
        X_test.to_pickle(self.processed_dir / "X_test.pkl")
        y_train.to_pickle(self.processed_dir / "y_train.pkl")
        y_val.to_pickle(self.processed_dir / "y_val.pkl")
        y_test.to_pickle(self.processed_dir / "y_test.pkl")
        
        # Sauvegarde scaler
        with open(self.models_dir / "preprocessor.pkl", 'wb') as f:
            pickle.dump(self.scaler, f)
        
        print(f"âœ… DonnÃ©es sauvegardÃ©es dans {self.processed_dir}")
        print(f"   â€¢ Train/Val pour training: X_train.pkl, X_val.pkl")
        print(f"   â€¢ Test ISOLÃ‰ pour Ã©valuation finale: X_test.pkl")
        print(f"âœ… Preprocessor sauvegardÃ© dans {self.models_dir}")
    
    def get_preprocessing_summary(self, X_train, X_val, X_test, y_train, y_val, y_test) -> dict:
        """RÃ©sumÃ© preprocessing pour logging"""
        total_samples = len(X_train) + len(X_val) + len(X_test)
        
        return {
            'total_samples': total_samples,
            'train_samples': len(X_train),
            'validation_samples': len(X_val),
            'test_samples': len(X_test),
            'features_count': X_train.shape[1],
            'train_fraud_rate': y_train.mean(),
            'validation_fraud_rate': y_val.mean(),
            'test_fraud_rate': y_test.mean(),
            'split_strategy': 'triple_split_anti_leakage',
            'preprocessing_steps': [
                'data_cleaning',
                'feature_engineering', 
                'triple_split_stratified',
                'scaling_on_train_only'
            ]
        }
    
    def run_full_pipeline(self):
        """Pipeline preprocessing complet - VERSION CORRIGÃ‰E"""
        print("ğŸš€ DÃ‰MARRAGE PIPELINE PREPROCESSING ANTI-LEAKAGE")
        print("=" * 70)
        
        try:
            # 1. Chargement
            df = self.load_raw_data()
            
            # 2. Nettoyage
            df_clean = self.clean_data(df)
            
            # 3. Feature Engineering
            df_engineered = self.feature_engineering(df_clean)
            
            # 4. ğŸ¯ NOUVEAU: Triple split AVANT normalisation
            X_train, X_val, X_test, y_train, y_val, y_test = self.triple_split_data(df_engineered)
            
            # 5. ğŸ¯ CRITIQUE: Normalisation sur train uniquement
            X_train_scaled, X_val_scaled, X_test_scaled = self.fit_scaler_on_train_only(
                X_train, X_val, X_test
            )
            
            # 6. Sauvegarde
            self.save_processed_data(
                X_train_scaled, X_val_scaled, X_test_scaled, 
                y_train, y_val, y_test
            )
            
            # 7. RÃ©sumÃ©
            summary = self.get_preprocessing_summary(
                X_train_scaled, X_val_scaled, X_test_scaled,
                y_train, y_val, y_test
            )
            
            print("\nğŸ‰ PREPROCESSING ANTI-LEAKAGE TERMINÃ‰")
            print("=" * 70)
            print(f"ğŸ“Š Train: {summary['train_samples']:,} ({summary['train_fraud_rate']*100:.2f}% fraud)")
            print(f"ğŸ“Š Validation: {summary['validation_samples']:,} ({summary['validation_fraud_rate']*100:.2f}% fraud)")
            print(f"ğŸ“Š Test: {summary['test_samples']:,} ({summary['test_fraud_rate']*100:.2f}% fraud)")
            print(f"ğŸ”¢ Features finales: {summary['features_count']}")
            print(f"ğŸ”’ Test set COMPLÃˆTEMENT ISOLÃ‰")
            print(f"âœ… DonnÃ©es prÃªtes pour training sans data leakage")
            
            return True
            
        except Exception as e:
            print(f"\nâŒ ERREUR PREPROCESSING: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ExÃ©cution pipeline preprocessing"""
    print("ğŸ¯ Credit Card Fraud Detection - Preprocessing Anti-Leakage")
    
    preprocessor = FraudDataPreprocessor(
        test_size=0.2,      # 20% pour test (isolÃ©)
        validation_size=0.2, # 20% pour validation
        random_state=42     # 60% pour train
    )
    
    success = preprocessor.run_full_pipeline()
    
    if success:
        print("\nâœ… PREPROCESSING RÃ‰USSI!")
        print("ğŸ¯ Prochaine Ã©tape: python src/model_training.py")
    else:
        print("\nâŒ PREPROCESSING Ã‰CHOUÃ‰!")
        exit(1)

if __name__ == "__main__":
    main()